---
title: "OpenAI Compatible Gateway"
description: "Connect to any OpenAI-compatible API endpoint"
---

The OpenAI Compatible gateway connects ChatJS to any endpoint that follows the [OpenAI API format](https://platform.openai.com/docs/api-reference). This is the fastest way to connect to local inference servers and alternative cloud providers.

## Compatible Providers

This gateway works with:

- [Ollama](https://ollama.com) — local models on your machine
- [LM Studio](https://lmstudio.ai) — local model management with a GUI
- [vLLM](https://docs.vllm.ai) — high-throughput self-hosted inference
- [Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service) — OpenAI models on Azure
- Any other endpoint that implements `/v1/chat/completions` and `/v1/models`

## Setup

1. Set the base URL and (optionally) an API key in `.env.local`:

```bash
OPENAI_COMPATIBLE_BASE_URL=http://localhost:11434/v1  # Ollama example
OPENAI_COMPATIBLE_API_KEY=                             # Optional, depends on provider
```

2. Set the gateway in your config:

```typescript title="chat.config.ts"
const config: ConfigInput = {
  models: {
    gateway: "openai-compatible",
    defaults: {
      chat: "llama3.2",
      title: "llama3.2",
      // ...
    },
    // ...
  },
};
```

## Authentication

| Variable | Description |
|----------|-------------|
| `OPENAI_COMPATIBLE_BASE_URL` | Required. The base URL of the API (e.g., `http://localhost:11434/v1`) |
| `OPENAI_COMPATIBLE_API_KEY` | Optional. API key if the provider requires authentication |

Without the base URL, the app falls back to the static model snapshot in `models.generated.ts` and cannot make API calls.

## Available Models

Models are fetched at runtime from `{OPENAI_COMPATIBLE_BASE_URL}/models` and cached for 1 hour. The available models depend entirely on your provider.

### Ollama

After [installing Ollama](https://ollama.com/download), pull models and they appear automatically:

```bash
ollama pull llama3.2
ollama pull codellama
```

### LM Studio

Download models through the LM Studio UI, then start the local server. Models appear at `http://localhost:1234/v1/models`.

```bash
OPENAI_COMPATIBLE_BASE_URL=http://localhost:1234/v1
```

## Image Generation

Image generation support depends on the provider. If the endpoint supports the OpenAI image generation API format, `createImageModel()` will work. Otherwise, multimodal language models with image output capabilities can still generate images inline.

## Key Files

| File | Purpose |
|------|---------|
| `lib/ai/gateways/openai-compatible-gateway.ts` | Gateway implementation |
| `lib/ai/gateways/gateway-provider.ts` | `GatewayProvider` interface |
| `lib/ai/gateways/registry.ts` | Gateway registry and resolution |

## Related

- [Gateways Overview](/gateways/overview) for the gateway system architecture
- [Adding a Gateway](/cookbook/adding-a-gateway) if you need more control than this generic gateway provides
- [Multi-Model Support](/core/multi-model) for model configuration
